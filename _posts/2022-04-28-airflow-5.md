---
title: "Airflow Celery Executor 시작하기"
excerpt: "celery로 worker에 여러 task 분배하기"
toc: true
toc_sticky: true
categories:
  - Airflow
  - DataPipeline
modified_date: 2020-04-28 15:13:28 +0900
---
- [Airflow 관련 포스팅 전체 보기](https://dasoldasol.github.io/airflow/datapipeline/airflow-linklist/)


## 개념 
![image](https://user-images.githubusercontent.com/29423260/165689392-f68abc42-af3d-4506-b463-8b690ac7bb09.png)
![image](https://user-images.githubusercontent.com/29423260/165689407-8ee78102-1f76-40a1-bac8-bbb2781472bb.png)
1. celery executor는 task를 외부 큐에 등록하고 원격 워커에서 실행하게 함
2. external tool (message broker) redis 설치 worker 
3. task를 구동할 수 있는 각 worker는 같은 dependency를 공유해야한다. 각 worker는 동일한 라이브러리등이 설치되어 있어야. 
- broker : 실행 명령을 저장 
- result_backend : celery task state 저장, 완료된 명령상태 저장 

## 환경 설정 
### celery 설치 
- web server/scheduler stop 
- ```pip install 'apache-airflow[celery]'```

### redis (message broker) 설치
```
  sudo apt update
  sudo apt install redis-server
```
- config 파일 수정     
```
  sudo nano /etc/redis/redis.conf
  supervised no => supervised systemd
```
- redis.service 실행    
```
  sudo systemctl restart redis.service
  sudo systemctl status redis.service <= active 확인 
```

### airflow.cfg 수정 
- airflow.cfg에서 executor 수정 
  - executor = CeleryExecutor
- airflow.cfg에서 redis 파라미터 수정 
  - broker_url = redis://localhost:6379/0
  - result_backend = result_backend = db+postgresql://postgres:postgres@localhost/postgres <= sql_alchemy_conn에서 앞부분만 수정

### provider install 
```pip install 'apache-airflow[redis]'```

### flower, worker 실행
- flower? : celery UI, worker 상태, queue 등 확인    
```
  airflow celery flower
  airflow celery worker 
```

## UI 확인 
- webserver(8080) : trigger 
- flower(5555) : worker 동작 확인 
