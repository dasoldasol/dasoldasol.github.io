## S3 CheatSheet 
### Features
- Key-value based object storage with unlimited storage, unlimited objects up to 5TB for the internet 
- **Object Level Storage**(not a Block Level Storage) and cannot be used to host OS or dynamic websites
- Durability by **redundanctly storing objects on multiple facilities within a region**
- SSL encryption of data in transit and data encryption at rest
- verifies the integrity of data using checksums and provides auto healing capability
- integrates with CloudTrail, CloudWatch and SNS for event notifications


### S3 resources
- consists of **bucket and objects** stored in the bucket which can be retrieved via a unique, developer-assigned key
- bucket names are **globally unique**
- **data model is a flat structure** with no hierarchies or folders
- **Logical hierarchy** can be inferred using the keyname prefix


### Bucket & Object Operations 
- allows **retrieval of 1000 objects** and provides **pagination** support and is **NOT** suited for list or prefix queries with large number of objects
- with a single put operations, 5GB size object can be uploaded
- use **Multipart upload** to upload large objects up to 5TB and is recommended for object size over 100MB for fault tolerant uploads
- support **Range HTTP Header** to retrieve partial objects for fault tolerant downloads where the network connectivity is poor
- **Pre-Signed URLs** can also be used shared for uploading/downloading objects for **limited time without requiring AWS security credentials**
- allows deletion of a single object or multiple objects(max 1000) in a single call 


### Multipart Uploads
- **parallel uploads** with improved throughput and bandwidth utilization
- **fault tolerance and quick recovery** from network issues 
- ability to **pause and resume** uploads 
- begin an upload before the final object size is known 


### Versioning
- allows preserve, retrieve, and restore every version of every object 
- **protects individual files** but does **NOT protect from Bucket deletion**


### Storage tiers 
- **Standard**
  - default storage class
  - **99.999999999% durability & 99.99% availability**
  - Low latency and high throughput performance 
  - designed to **sustain the loss of data in two** facilities 
- **Standard IA**
  - optimized for **long-lived and less frequently** accessed data 
  - designed to **sustain the loss of data in two** facilities
  - **99.999999999% durability & 99.9% availability**
  - suitable for objects **greater than 128KB** kept for at **least 30 days**
- **Reduced Redundancy Storage**
  - designed for **noncritival, reproducible data** stored at lower levels of redundancy than the STANDARD storage class
  - reduces storage costs
  - **99.99% durability & 99.99% availability**
  - designed to **sustain the loss of data in a single** facility
- **Glacier**
  - suitable for **archiving data** where **data access is infrequent and retrieval time of several(3-5) hours** is acceptable
  - **99.999999999% durability**
  
  
### Lifecycle Management Policies 
- **Transition** to move objects to different storage classes and Glacier
- **Expiration** to remove objects


### Data Consistency Model
- **Read-After-Write Consistency** : PUTS of new objects
- **Eventual Consistency** : overwrite PUTS and DELETES
- for new objects, **synchronously stores data across multiple facilities** before returning success
- **updates** to a single key are **atomic**


### Security 
- **IAM policies** : grant users within your own AWS account permission to access S3 resources 
- **Bucket and Object ACL** : grant other AWS accounts(not specific users) access to S3 resources
- **Bucket policies** : allows to add or deny permissions across some or all of the objects within a single bucket


### Best Practices 
- **Use random hash prefix for keys and ensure a random access pattern**, as S3 stores object lexicographically randomness helps distribute the contents across multiple partitions for better performance 
- Use parallel threads and **Multipart upload for faster writes**
- Use parallel threads and **Range Header GET for faster reads**
- for list operations with large number of objects, its better to build a secondary index in Dynamo DB
- Use **Versioning to protect from unintented overwrites and deletions**, but this does not protect against bucket deletion
- Use **VPC S3 Endpoints** with VPC to transfer data using Amazon internet network

## Features
### Concept
- object based : object - file / key - file  name /value - data => not suitable to install an operating system(os) or dynamic website
- MFA Delete : not every body can delete when you turn on 
- universal namespace : unique name 
- data consitency : write and read(PUTS), eventual(UPDATE, DELETE) 
- gurantee : 11x9 durability 
- S3 storage classes 
  - s3 Standard 
  - s3 IA (Infrequent Access)
  - s3 One Zone IA 
  - s3 Intelligent Tiering : maximize cost saving 
  - s3 Glacier : retrieval time 
  - s3 Glacier Deep Archive 
- READ [S3 FAQ](https://aws.amazon.com/ko/s3/faqs/)!!!!!!!!!!

### Creating Bucket
- tags ex. key-team value-marketing team 
- block all public access by default 
- file upload success http 200
- object access denied , make public 
  - edit public setting (uncheck) -> object actions make public 
- storage class (change classes)
- Control access to buckets using Bucket ACL or Bucket Policies 

### Security and Encryption
- in transit : SSL/TLS ex. https 
- server side(AWS manage)
	- S3 Managed Keys : SSE-S3
	- AWS Key Management Service(custom + aws) : SSE-KMS
	- customized : SSE-C
- client side encryption : you do it 

### Version Control
- cannot be disabled , lifecycle
- properties : versioning 
- uploading new file : have to change the permission (actions - make public)
- delete new version file : delete marker 
- stores all versions (even if you delete)
- MFA Delete capability 

### LifeCycle Management
- automates moving to storage tiers 
- can be used with versioning 
- can be applied to current & previous versions 

### Cross Region Replication(CRR)
- Management - replication 
- enable versioning 
- regions must be unique
- existing files NOT replicated 
- updated files replicated
- delete marker/delete NOT replicated : have to delete manually 

### Transfer Accerlation - Edge Location
- s3 Transfer Accerlation Tool (Speed Comparision)
- [CloudFront](https://github.com/dasoldasol/dasolseo.github.io/blob/master/_posts/CloudFront.md)

## Scenarios
- You have been asked to advise on a scaling concern. The client has an elegant solution that works well. As the information base grows they use CloudFormation to spin up another stack made up of an S3 bucket and supporting compute instances. The trigger for creating a new stack is when the PUT rate approaches 100 PUTs per second. The problem is that as the business grows that number of buckets is growing into the hundreds and will soon be in the thousands. You have been asked what can be done to reduce the number of buckets without changing the basic architecture.
	- **Change the trigger level to around 3000 as s3 can now accommodate much higher PUT and GET levels**
	- Until 2018 there was a hard limit on S3 puts of 100 PUTs per second. To achieve this care needed to be taken with the structure of the name Key to ensure parallel processing. As of July 2018 the limit was raised to 3500 and the need for the Key design was basically eliminated.

- You run a meme creation website where users can create memes and then download them for use on their own sites. The original images are stored in S3 and each meme's metadata in DynamoDB. You need to decide upon a low-cost storage option for the memes, themselves. If a meme object is unavailable or lost, a Lambda function will automatically recreate it using the original file from S3 and the metadata from DynamoDB. Which storage solution should you use to store the non-critical, easily reproducible memes in the most cost-effective way?
	- **S3-OneZone-IA**
	- S3 – OneZone-IA is the recommended storage for when you want cheaper storage for infrequently accessed objects. It has the same durability but less availability. There can be cost implications if you use it frequently or use it for short lived storage. Glacier is cheaper, but has a long retrieval time. RRS has effectively been deprecated. It still exists but is not a service that AWS want to sell anymore.

- You work for a health insurance company that amasses a large number of patients' health records. Each record will be used once when assessing a customer, and will then need to be securely stored for a period of 7 years. In some rare cases, you may need to retrieve this data within 24 hours of a claim being lodged. Given these requirements, which type of AWS storage would deliver the least expensive solution?
	- **Glacier**
	- The recovery rate is a key decider. The record shortage must be; safe, durable, low cost, and the recovery can be slow. All features of Glacier.

- You run a popular photo-sharing website that depends on S3 to store content. Paid advertising is your primary source of revenue. However, you have discovered that other websites are linking directly to the images in your buckets, not to the HTML pages that serve the content. This means that people are not seeing the paid advertising, and you are paying AWS unnecessarily to serve content directly from S3. How might you resolve this issue?
	- **Remove the ability for images to be served public to the site and then use Signed URLs with expiry dates**

- You work for a major news network in Europe. They have just released a new mobile app that allows users to post their photos of newsworthy events in real-time, which are then reviewed by your editors before being copied to your website and made public. Your organization expects this app to grow very quickly, essentially doubling its user base each month. The app uses S3 to store the images, and you are expecting sudden and sizable increases in traffic to S3 when a major news event takes place (as users will be uploading large amounts of content.) You need to keep your storage costs to a minimum, and it does not matter if some objects are lost. With these factors in mind, which storage media should you use to keep costs as low as possible?
	- **S3-OneZone-IA**
	- The key driver here is cost, so an awareness of cost is necessary to answer this. 
		- S3-RRS($0.024/GB)>S3($0.023)>S3 standard IA($0.0125)>S3 One-Zone-IA($0.01)>Glacier($0.004)
	- Glacier cannot be considered as it is not intended for direct access. Of course you spotted that RRS is being deprecated, and there is no such thing as S3 – Provisioned IOPS. In this case OneZone

- You work for a busy digital marketing company who currently store their data on-premise. They are looking to migrate to AWS S3 and to store their data in buckets. Each bucket will be named after their individual customers, followed by a random series of letters and numbers. Once written to S3 the data is rarely changed, as it has already been sent to the end customer for them to use as they see fit. However, on some occasions, customers may need certain files updated quickly, and this may be for work that has been done months or even years ago. You would need to be able to access this data immediately to make changes in that case, but you must also keep your storage costs extremely low. The data is not easily reproducible if lost. Which S3 storage class should you choose to minimize costs and to maximize retrieval times?
	- **S3-IA**
	- The need to immediate access is an important requirement along with cost. Glacier has a long recovery time at a low cost or a shorter recovery time at a high cost, and 1Zone-IA has a lower Availability level which means that it may not be available when needed.

- You work for a major news network in Europe. They have just released a new mobile app that allows users to post their photos of newsworthy events in real-time. Your organization expects this app to grow very quickly, essentially doubling its user base each month. The app uses S3 to store the images, and you are expecting sudden and sizable increases in traffic to S3 when a major news event takes place (as users will be uploading large amounts of content.) You need to keep your storage costs to a minimum, and **you are happy to temporally lose access to up to 0.1% of uploads per year**. With these factors in mind, which storage media should you use to keep costs as low as possible?
	- **S3-IA**
	- S3-RRS($0.024/GB)>S3($0.023)>S3 standard IA($0.0125)>S3 One-Zone-IA($0.01)>Glacier($0.004)
	- Glacier cannot be considered as it is not intended for direct access. 3 has an availability of 99.99%, S3-IA has an availability of 99.9% while S3-1Zone-IA only has 99.5%.
