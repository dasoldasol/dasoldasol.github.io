## S3 CheatSheet 
### Features
- Key-value based object storage with unlimited storage, unlimited objects up to 5TB for the internet 
- **Object Level Storage**(not a Block Level Storage) and cannot be used to host OS or dynamic websites
- Durability by **redundanctly storing objects on multiple facilities within a region**
- SSL encryption of data in transit and data encryption at rest
- verifies the integrity of data using checksums and provides auto healing capability
- integrates with CloudTrail, CloudWatch and SNS for event notifications

### S3 resources
- consists of **bucket and objects** stored in the bucket which can be retrieved via a unique, developer-assigned key
- bucket names are **globally unique**
- **data model is a flat structure** with no hierarchies or folders
- **Logical hierarchy** can be inferred using the keyname prefix

### Bucket & Object Operations 
- allows **retrieval of 1000 objects** and provides **pagination** support and is **NOT** suited for list or prefix queries with large number of objects
- with a single put operations, 5GB size object can be uploaded
- use **Multipart upload** to upload large objects up to 5TB and is recommended for object size over 100MB for fault tolerant uploads
- support **Range HTTP Header** to retrieve partial objects for fault tolerant downloads where the network connectivity is poor
- **Pre-Signed URLs** can also be used shared for uploading/downloading objects for **limited time without requiring AWS security credentials**
- allows deletion of a single object or multiple objects(max 1000) in a single call 

### Multipart Uploads
- **parallel uploads** with improved throughput and bandwidth utilization
- **fault tolerance and quick recovery** from network issues 
- ability to **pause and resume** uploads 
- begin an upload before the final object size is known 

### Versioning
- allows preserve, retrieve, and restore every version of every object 
- **protects individual files** but does **NOT protect from Bucket deletion**

### Storage tiers 
- **Standard**
  - default storage class
  - **99.999999999% durability & 99.99% availability**
  - Low latency and high throughput performance 
  - designed to **sustain the loss of data in two** facilities 
- **Standard IA**
  - optimized for **long-lived and less frequently** accessed data 
  - designed to **sustain the loss of data in two** facilities
  - **99.999999999% durability & 99.9% availability**
  - suitable for objects **greater than 128KB** kept for at **least 30 days**
- **Reduced Redundancy Storage**
  - designed for **noncritival, reproducible data** stored at lower levels of redundancy than the STANDARD storage class
  - reduces storage costs
  - **99.99% durability & 99.99% availability**
  - designed to **sustain the loss of data in a single** facility
- **Glacier**
  - suitable for **archiving data** where **data access is infrequent and retrieval time of several(3-5) hours** is acceptable
  - **99.999999999% durability**

### Lifecycle Management Policies 
- **Transition** to move objects to different storage classes and Glacier
- **Expiration** to remove objects

### Data Consistency Model
- **Read-After-Write Consistency** : PUTS of new objects
- **Eventual Consistency** : overwrite PUTS and DELETES
- for new objects, **synchronously stores data across multiple facilities** before returning success
- **updates** to a single key are **atomic**

### Security 
- **IAM policies** : grant users within your own AWS account permission to access S3 resources 
- **Bucket and Object ACL** : grant other AWS accounts(not specific users) access to S3 resources
- **Bucket policies** : allows to add or deny permissions across some or all of the objects within a single bucket

### Best Practices 
- **Use random hash prefix for keys and ensure a random access pattern**, as S3 stores object lexicographically randomness helps distribute the contents across multiple partitions for better performance 
- Use parallel threads and **Multipart upload for faster writes**
- Use parallel threads and **Range Header GET for faster reads**
- for list operations with large number of objects, its better to build a secondary index in Dynamo DB
- Use **Versioning to protect from unintented overwrites and deletions**, but this does not protect against bucket deletion
- Use **VPC S3 Endpoints** with VPC to transfer data using Amazon internet network

## Features
### Concept
- object based : object - file / key - file  name /value - data => not suitable to install an operating system(os) or dynamic website
- MFA Delete : not every body can delete when you turn on 
- universal namespace : unique name 
- data consitency : write and read(PUTS), eventual(UPDATE, DELETE) 
- gurantee : 11x9 durability 
- S3 storage classes 
  - s3 Standard 
  - s3 IA (Infrequent Access)
  - s3 One Zone IA 
  - s3 Intelligent Tiering : maximize cost saving 
  - s3 Glacier : retrieval time 
  - s3 Glacier Deep Archive 
- READ [S3 FAQ](https://aws.amazon.com/ko/s3/faqs/)!!!!!!!!!!

### Creating Bucket
- tags ex. key-team value-marketing team 
- block all public access by default 
- file upload success http 200
- object access denied , make public 
  - edit public setting (uncheck) -> object actions make public 
- storage class (change classes)
- Control access to buckets using Bucket ACL or Bucket Policies 

### Security and Encryption
- in transit : SSL/TLS ex. https 
- server side(AWS manage)
	- S3 Managed Keys : SSE-S3
	- AWS Key Management Service(custom + aws) : SSE-KMS
	- customized : SSE-C
- client side encryption : you do it 

### Version Control
- cannot be disabled , lifecycle
- properties : versioning 
- uploading new file : have to change the permission (actions - make public)
- delete new version file : delete marker 
- stores all versions (even if you delete)
- MFA Delete capability 

### LifeCycle Management
- automates moving to storage tiers 
- can be used with versioning 
- can be applied to current & previous versions 

### Cross Region Replication(CRR)
- Management - replication 
- enable versioning 
- regions must be unique
- existing files NOT replicated 
- updated files replicated
- delete marker/delete NOT replicated : have to delete manually 

### Transfer Accerlation - Edge Location
- s3 Transfer Accerlation Tool (Speed Comparision)
- [CloudFront](https://github.com/dasoldasol/dasolseo.github.io/blob/master/_posts/CloudFront.md)

## Scenarios
- **Company salespeople upload their sales figures daily. A Solutions Architect needs a durable storage solution for these documents that also protects against users accidentally deleting important documents.   
Which action will protect against unintended user actions?**   
   
	A. Store data in an EBS volume and create snapshots once a week.    
	**B. Store data in an S3 bucket and enable versioning.**   
	C. Store data in two S3 buckets in different AWS regions.   
	D. Store data on EC2 instance storage.   
	   
	   - If a versioned object is deleted, then it can still be recored by retrieving the final version
	   - Taking snapshots would lose any changed committed since the previous snapshot
	   - Storing data in 2 buckets : user could still delete the object from both buckets
	   - EC2 instance storage is ephemeral and should never be used for data requiring durability.

- **An application saves the logs to an S3 bucket. A user wants to keep the logs for one month for troubleshooting purposes, and then purge the logs.    
What feature will enable this?**
	- **Configuring lifecycle configuration rules on the S3 bucket.**
	- Lifecycle configuration : allows lifecycle management of objects in a bucket. The configuration is a set of one or more rules, where each rule defines an action for Amazon S3 to apply to a group of objects. 
	- Bucket policies & IAM : define access to objects in an S3 bucket
	- CORS : enables client in one domain to interact with resources in a different domain.

- You have been asked to advise on a scaling concern. The client has an elegant solution that works well. As the information base grows they use CloudFormation to spin up another stack made up of an S3 bucket and supporting compute instances. The trigger for creating a new stack is when the PUT rate approaches 100 PUTs per second. The problem is that as the business grows that number of buckets is growing into the hundreds and will soon be in the thousands. You have been asked what can be done to reduce the number of buckets without changing the basic architecture.
	- **Change the trigger level to around 3000 as s3 can now accommodate much higher PUT and GET levels**
	- Until 2018 there was a hard limit on S3 puts of 100 PUTs per second. To achieve this care needed to be taken with the structure of the name Key to ensure parallel processing. As of July 2018 the limit was raised to 3500 and the need for the Key design was basically eliminated.

- You run a meme creation website where users can create memes and then download them for use on their own sites. The original images are stored in S3 and each meme's metadata in DynamoDB. You need to decide upon a low-cost storage option for the memes, themselves. If a meme object is unavailable or lost, a Lambda function will automatically recreate it using the original file from S3 and the metadata from DynamoDB. Which storage solution should you use to store the non-critical, easily reproducible memes in the most cost-effective way?
	- **S3-OneZone-IA**
	- S3 â€“ OneZone-IA is the recommended storage for when you want cheaper storage for infrequently accessed objects. It has the same durability but less availability. There can be cost implications if you use it frequently or use it for short lived storage. Glacier is cheaper, but has a long retrieval time. RRS has effectively been deprecated. It still exists but is not a service that AWS want to sell anymore.

- You work for a health insurance company that amasses a large number of patients' health records. Each record will be used once when assessing a customer, and will then need to be securely stored for a period of 7 years. In some rare cases, you may need to retrieve this data within 24 hours of a claim being lodged. Given these requirements, which type of AWS storage would deliver the least expensive solution?
	- **Glacier**
	- The recovery rate is a key decider. The record shortage must be; safe, durable, low cost, and the recovery can be slow. All features of Glacier.

- You run a popular photo-sharing website that depends on S3 to store content. Paid advertising is your primary source of revenue. However, you have discovered that other websites are linking directly to the images in your buckets, not to the HTML pages that serve the content. This means that people are not seeing the paid advertising, and you are paying AWS unnecessarily to serve content directly from S3. How might you resolve this issue?
	- **Remove the ability for images to be served public to the site and then use Signed URLs with expiry dates**

- You work for a major news network in Europe. They have just released a new mobile app that allows users to post their photos of newsworthy events in real-time, which are then reviewed by your editors before being copied to your website and made public. Your organization expects this app to grow very quickly, essentially doubling its user base each month. The app uses S3 to store the images, and you are expecting sudden and sizable increases in traffic to S3 when a major news event takes place (as users will be uploading large amounts of content.) You need to keep your storage costs to a minimum, and it does not matter if some objects are lost. With these factors in mind, which storage media should you use to keep costs as low as possible?
	- **S3-OneZone-IA**
	- The key driver here is cost, so an awareness of cost is necessary to answer this. 
		- S3-RRS($0.024/GB)>S3($0.023)>S3 standard IA($0.0125)>S3 One-Zone-IA($0.01)>Glacier($0.004)
	- Glacier cannot be considered as it is not intended for direct access. Of course you spotted that RRS is being deprecated, and there is no such thing as S3 â€“ Provisioned IOPS. In this case OneZone

- You work for a busy digital marketing company who currently store their data on-premise. They are looking to migrate to AWS S3 and to store their data in buckets. Each bucket will be named after their individual customers, followed by a random series of letters and numbers. Once written to S3 the data is rarely changed, as it has already been sent to the end customer for them to use as they see fit. However, on some occasions, customers may need certain files updated quickly, and this may be for work that has been done months or even years ago. You would need to be able to access this data immediately to make changes in that case, but you must also keep your storage costs extremely low. The data is not easily reproducible if lost. Which S3 storage class should you choose to minimize costs and to maximize retrieval times?
	- **S3-IA**
	- The need to immediate access is an important requirement along with cost. Glacier has a long recovery time at a low cost or a shorter recovery time at a high cost, and 1Zone-IA has a lower Availability level which means that it may not be available when needed.

- You work for a major news network in Europe. They have just released a new mobile app that allows users to post their photos of newsworthy events in real-time. Your organization expects this app to grow very quickly, essentially doubling its user base each month. The app uses S3 to store the images, and you are expecting sudden and sizable increases in traffic to S3 when a major news event takes place (as users will be uploading large amounts of content.) You need to keep your storage costs to a minimum, and **you are happy to temporally lose access to up to 0.1% of uploads per year**. With these factors in mind, which storage media should you use to keep costs as low as possible?
	- **S3-IA**
	- S3-RRS($0.024/GB)>S3($0.023)>S3 standard IA($0.0125)>S3 One-Zone-IA($0.01)>Glacier($0.004)
	- Glacier cannot be considered as it is not intended for direct access. 3 has an availability of 99.99%, S3-IA has an availability of 99.9% while S3-1Zone-IA only has 99.5%.

- You work for a manufacturing company that operate a hybrid infrastructure with systems located both in a local data center and in AWS, connected via AWS Direct Connect. Currently, all **on-premise** servers are backed up to a local NAS, but your CTO wants you to decide on the best way to **store copies of these backups** in AWS. He has asked you to propose a solution which will provide **access to the files within milliseconds** should they be needed, but at the same time minimizes cost. As these files will be copies of backups stored on-premise, **availability is not as critical as durability.** Choose the best option from the following which meets the brief.
	- **S3-IA**
	- Cost : S3-RRS($0.024/GB)>S3($0.023)>S3 standard IA($0.0125)>S3 One-Zone-IA($0.01)>Glacier($0.004)
	- Duralility : ALL SAME 11 9's
	- Availability : S3(99.99%)=S3-IA(99.9%)>S3 OneZone-IA(99.5%)

- You need to use an object-based storage solution to store your critical, **non-replaceable** data in a cost-effective way. This data will be **frequently updated** and will need some form of version control enabled on it. Which S3 storage solution should you use?
	- **S3**
	-  The 1st excludes anything the has reduced durability, the second excluded anything with long recall, reduced availability, or billing based on infrequent access.

- You have launched a travel photo sharing website using Amazon S3 to serve high-quality photos to visitors of your website. After a few days, you found out that there are **other travel websites linking and using your photos**. This resulted in financial losses for your business.    
What is an effective method to mitigate this issue?
	- **A) Configure your S3 bucket to remove public read access and use pre-signed URLs with expiry dates.**

- A Solutions Architect is designing an online medical system in AWS which will store sensitive Personally Identifiable Information (PII) of the users in an Amazon S3 bucket. **Both the master keys and the unencrypted data should never be sent to AWS** to comply with the strict compliance and regulatory requirements of the company.    
Which **S3 encryption technique** should the Architect use?
	- **A) Use S3 client-side encryption with a client-side master key.**
	- Client-side encryption is the act of encrypting data **before** sending it to Amazon S3.
	- Using S3 client-side encryption with a KMS-managed customer master key : is incorrect because in client-side encryption with a KMS-managed customer master key, you provide an AWS KMS customer master key ID (CMK ID) to AWS.
	- Using S3 server-side encryption with customer provided key **(SSE-C)** : is incorrect. you have to use client-side encryption in order to encrypt the data first before sending to AWS. For the S3 server-side encryption with customer-provided key (SSE-C), you actually provide the encryption key as part of your request to upload the object to S3. Using this key, Amazon S3 manages both the encryption (as it writes to disks) and decryption (when you access your objects).

- A Solutions Architect is hosting a website in an Amazon S3 bucket named `tutorialsdojo`. The users load the website using the following URL: `http://tutorialsdojo.s3-website-us-east-1.amazonaws.com` and there is a new requirement to add a JavaScript on the webpages in order to make authenticated HTTP GET requests against the same bucket by using the Amazon S3 API endpoint (`tutorialsdojo.s3.amazonaws.com`). Upon testing, you noticed that the **web browser blocks JavaScript from allowing those requests**.    
Which of the following options is the MOST suitable solution that you should implement for this scenario?
	- **A) Enable Cross-origin resource sharing(CORS) configuration in the bucket**

- In a government agency that you are working for, you have been assigned to put confidential tax documents on AWS cloud. However, there is a concern from a security perspective on what can be put on AWS.     
What are the features in AWS that can ensure **data security for your confidential documents?** (Choose 2)
	- **S3 client-side and server-side encryption**

- You are working for an advertising company as their Senior Solutions Architect handling the S3 storage data. Your company has terabytes of data sitting on AWS S3 standard storage class, which accumulates significant operational costs. The management wants to cut down on the cost of their cloud infrastructure so you were instructed to switch to Glacier to lessen the cost per GB storage.    
The Amazon **Glacier storage service is primarily used for** which use case? (Choose 2) 
	- **A1) Storing Data archives**
	- **A2) Storing infrequently accessed data**
	- **Storing cached session data** : is incorrect because this is the main use case for **ElastiCache**
	- **Used as a data warehouse** : is incorrect because data warehousing is the main use case of **Amazon Redshift**.
	
