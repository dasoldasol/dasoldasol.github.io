## S3 CheatSheet 
### Features
- Key-value based object storage with unlimited storage, unlimited objects up to 5TB for the internet 
- **Object Level Storage**(not a Block Level Storage) and cannot be used to host OS or dynamic websites
- Durability by **redundanctly storing objects on multiple facilities within a region**
- SSL encryption of data in transit and data encryption at rest
- verifies the integrity of data using checksums and provides auto healing capability
- integrates with CloudTrail, CloudWatch and SNS for event notifications


### S3 resources
- consists of **bucket and objects** stored in the bucket which can be retrieved via a unique, developer-assigned key
- bucket names are **globally unique**
- **data model is a flat structure** with no hierarchies or folders
- **Logical hierarchy** can be inferred using the keyname prefix


### Bucket & Object Operations 
- allows **retrieval of 1000 objects** and provides **pagination** support and is **NOT** suited for list or prefix queries with large number of objects
- with a single put operations, 5GB size object can be uploaded
- use **Multipart upload** to upload large objects up to 5TB and is recommended for object size over 100MB for fault tolerant uploads
- support **Range HTTP Header** to retrieve partial objects for fault tolerant downloads where the network connectivity is poor
- **Pre-Signed URLs** can also be used shared for uploading/downloading objects for **limited time without requiring AWS security credentials**
- allows deletion of a single object or multiple objects(max 1000) in a single call 


### Multipart Uploads
- **parallel uploads** with improved throughput and bandwidth utilization
- **fault tolerance and quick recovery** from network issues 
- ability to **pause and resume** uploads 
- begin an upload before the final object size is known 


### Versioning
- allows preserve, retrieve, and restore every version of every object 
- **protects individual files** but does **NOT protect from Bucket deletion**


### Storage tiers 
- **Standard**
  - default storage class
  - **99.999999999% durability & 99.99% availability**
  - Low latency and high throughput performance 
  - designed to **sustain the loss of data in two** facilities 
- **Standard IA**
  - optimized for **long-lived and less frequently** accessed data 
  - designed to **sustain the loss of data in two** facilities
  - **99.999999999% durability & 99.9% availability**
  - suitable for objects **greater than 128KB** kept for at **least 30 days**
- **Reduced Redundancy Storage**
  - designed for **noncritival, reproducible data** stored at lower levels of redundancy than the STANDARD storage class
  - reduces storage costs
  - **99.99% durability & 99.99% availability**
  - designed to **sustain the loss of data in a single** facility
- **Glacier**
  - suitable for **archiving data** where **data access is infrequent and retrieval time of several(3-5) hours** is acceptable
  - **99.999999999% durability**
  
  
### Lifecycle Management Policies 
- **Transition** to move objects to different storage classes and Glacier
- **Expiration** to remove objects


### Data Consistency Model
- **Read-After-Write Consistency** : PUTS of new objects
- **Eventual Consistency** : overwrite PUTS and DELETES
- for new objects, **synchronously stores data across multiple facilities** before returning success
- **updates** to a single key are **atomic**


### Security 
- **IAM policies** : grant users within your own AWS account permission to access S3 resources 
- **Bucket and Object ACL** : grant other AWS accounts(not specific users) access to S3 resources
- **Bucket policies** : allows to add or deny permissions across some or all of the objects within a single bucket


## Best Practices 
- **Use random hash prefix for keys and ensure a random access pattern**, as S3 stores object lexicographically randomness helps distribute the contents across multiple partitions for better performance 
- Use parallel threads and **Multipart upload for faster writes**
- Use parallel threads and **Range Header GET for faster reads**
- for list operations with large number of objects, its better to build a secondary index in Dynamo DB
- Use **Versioning to protect from unintented overwrites and deletions**, but this does not protect against bucket deletion
- Use **VPC S3 Endpoints** with VPC to transfer data using Amazon internet network
