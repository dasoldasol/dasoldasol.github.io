## S3 CheatSheet 
### Features
- Key-value based object storage with unlimited storage, unlimited objects up to 5TB for the internet 
- **Object Level Storage**(not a Block Level Storage) and cannot be used to host OS or dynamic websites
- Durability by **redundanctly storing objects on multiple facilities within a region**
- SSL encryption of data in transit and data encryption at rest
- verifies the integrity of data using checksums and provides auto healing capability
- integrates with CloudTrail, CloudWatch and SNS for event notifications

### S3 resources
- consists of **bucket and objects** stored in the bucket which can be retrieved via a unique, developer-assigned key
- bucket names are **globally unique**
- **data model is a flat structure** with no hierarchies or folders
- **Logical hierarchy** can be inferred using the keyname prefix

### Bucket & Object Operations 
- allows **retrieval of 1000 objects** and provides **pagination** support and is **NOT** suited for list or prefix queries with large number of objects
- with a single put operations, 5GB size object can be uploaded
- use **Multipart upload** to upload large objects up to 5TB and is recommended for object size over 100MB for fault tolerant uploads
- support **Range HTTP Header** to retrieve partial objects for fault tolerant downloads where the network connectivity is poor
- **Pre-Signed URLs** can also be used shared for uploading/downloading objects for **limited time without requiring AWS security credentials**
- allows deletion of a single object or multiple objects(max 1000) in a single call 

### Multipart Uploads
- **parallel uploads** with improved throughput and bandwidth utilization
- **fault tolerance and quick recovery** from network issues 
- ability to **pause and resume** uploads 
- begin an upload before the final object size is known 

### Versioning
- allows preserve, retrieve, and restore every version of every object 
- **protects individual files** but does **NOT protect from Bucket deletion**

### Storage tiers 
- **Standard**
  - default storage class
  - **99.999999999% durability & 99.99% availability**
  - Low latency and high throughput performance 
  - designed to **sustain the loss of data in two** facilities 
- **Standard IA**
  - optimized for **long-lived and less frequently** accessed data 
  - designed to **sustain the loss of data in two** facilities
  - **99.999999999% durability & 99.9% availability**
  - suitable for objects **greater than 128KB** kept for at **least 30 days**
- **Reduced Redundancy Storage**
  - designed for **noncritival, reproducible data** stored at lower levels of redundancy than the STANDARD storage class
  - reduces storage costs
  - **99.99% durability & 99.99% availability**
  - designed to **sustain the loss of data in a single** facility
- **Glacier**
  - suitable for **archiving data** where **data access is infrequent and retrieval time of several(3-5) hours** is acceptable
  - **99.999999999% durability**

### Lifecycle Management Policies 
- **Transition** to move objects to different storage classes and Glacier
- **Expiration** to remove objects

### Data Consistency Model
- **Read-After-Write Consistency** : PUTS of new objects
- **Eventual Consistency** : overwrite PUTS and DELETES
- for new objects, **synchronously stores data across multiple facilities** before returning success
- **updates** to a single key are **atomic**

### Security 
- **IAM policies** : grant users within your own AWS account permission to access S3 resources 
- **Bucket and Object ACL** : grant other AWS accounts(not specific users) access to S3 resources
- **Bucket policies** : allows to add or deny permissions across some or all of the objects within a single bucket

### Best Practices 
- **Use random hash prefix for keys and ensure a random access pattern**, as S3 stores object lexicographically randomness helps distribute the contents across multiple partitions for better performance 
- Use parallel threads and **Multipart upload for faster writes**
- Use parallel threads and **Range Header GET for faster reads**
- for list operations with large number of objects, its better to build a secondary index in Dynamo DB
- Use **Versioning to protect from unintented overwrites and deletions**, but this does not protect against bucket deletion
- Use **VPC S3 Endpoints** with VPC to transfer data using Amazon internet network

## Features
### Concept
- object based : object - file / key - file  name /value - data => not suitable to install an operating system(os) or dynamic website
- MFA Delete : not every body can delete when you turn on 
- universal namespace : unique name 
- data consitency : write and read(PUTS), eventual(UPDATE, DELETE) 
- gurantee : 11x9 durability 
- S3 storage classes 
  - s3 Standard 
  - s3 IA (Infrequent Access)
  - s3 One Zone IA 
  - s3 Intelligent Tiering : maximize cost saving 
  - s3 Glacier : retrieval time 
  - s3 Glacier Deep Archive 
- READ [S3 FAQ](https://aws.amazon.com/ko/s3/faqs/)!!!!!!!!!!

### Creating Bucket
- tags ex. key-team value-marketing team 
- block all public access by default 
- file upload success http 200
- object access denied , make public 
  - edit public setting (uncheck) -> object actions make public 
- storage class (change classes)
- Control access to buckets using Bucket ACL or Bucket Policies 

### Security and Encryption
- in transit : SSL/TLS ex. https 
- server side(AWS manage)
	- S3 Managed Keys : SSE-S3
	- AWS Key Management Service(custom + aws) : SSE-KMS
	- customized : SSE-C
- client side encryption : you do it 

### Version Control
- cannot be disabled , lifecycle
- properties : versioning 
- uploading new file : have to change the permission (actions - make public)
- delete new version file : delete marker 
- stores all versions (even if you delete)
- MFA Delete capability 

### LifeCycle Management
- automates moving to storage tiers 
- can be used with versioning 
- can be applied to current & previous versions 

### Cross Region Replication(CRR)
- Management - replication 
- enable versioning 
- regions must be unique
- existing files NOT replicated 
- updated files replicated
- delete marker/delete NOT replicated : have to delete manually 

### Transfer Accerlation - Edge Location
- s3 Transfer Accerlation Tool (Speed Comparision)
- [CloudFront](https://github.com/dasoldasol/dasolseo.github.io/blob/master/_posts/CloudFront.md)

## Scenarios
- **Company salespeople upload their sales figures daily. A Solutions Architect needs a durable storage solution for these documents that also protects against users accidentally deleting important documents.   
Which action will protect against unintended user actions?**   
   
	A. Store data in an EBS volume and create snapshots once a week.    
	**B. Store data in an S3 bucket and enable versioning.**   
	C. Store data in two S3 buckets in different AWS regions.   
	D. Store data on EC2 instance storage.   
	   
	   - If a versioned object is deleted, then it can still be recored by retrieving the final version
	   - Taking snapshots would lose any changed committed since the previous snapshot
	   - Storing data in 2 buckets : user could still delete the object from both buckets
	   - EC2 instance storage is ephemeral and should never be used for data requiring durability.

- **An application saves the logs to an S3 bucket. A user wants to keep the logs for one month for troubleshooting purposes, and then purge the logs.    
What feature will enable this?**
	- **Configuring lifecycle configuration rules on the S3 bucket.**
	- Lifecycle configuration : allows lifecycle management of objects in a bucket. The configuration is a set of one or more rules, where each rule defines an action for Amazon S3 to apply to a group of objects. 
	- Bucket policies & IAM : define access to objects in an S3 bucket
	- CORS : enables client in one domain to interact with resources in a different domain.

- You have been asked to advise on a scaling concern. The client has an elegant solution that works well. As the information base grows they use CloudFormation to spin up another stack made up of an S3 bucket and supporting compute instances. The trigger for creating a new stack is when the PUT rate approaches 100 PUTs per second. The problem is that as the business grows that number of buckets is growing into the hundreds and will soon be in the thousands. You have been asked what can be done to reduce the number of buckets without changing the basic architecture.
	- **Change the trigger level to around 3000 as s3 can now accommodate much higher PUT and GET levels**
	- Until 2018 there was a hard limit on S3 puts of 100 PUTs per second. To achieve this care needed to be taken with the structure of the name Key to ensure parallel processing. As of July 2018 the limit was raised to 3500 and the need for the Key design was basically eliminated.

- You run a meme creation website where users can create memes and then download them for use on their own sites. The original images are stored in S3 and each meme's metadata in DynamoDB. You need to decide upon a low-cost storage option for the memes, themselves. If a meme object is unavailable or lost, a Lambda function will automatically recreate it using the original file from S3 and the metadata from DynamoDB. Which storage solution should you use to store the non-critical, easily reproducible memes in the most cost-effective way?
	- **S3-OneZone-IA**
	- S3 – OneZone-IA is the recommended storage for when you want cheaper storage for infrequently accessed objects. It has the same durability but less availability. There can be cost implications if you use it frequently or use it for short lived storage. Glacier is cheaper, but has a long retrieval time. RRS has effectively been deprecated. It still exists but is not a service that AWS want to sell anymore.

- You work for a health insurance company that amasses a large number of patients' health records. Each record will be used once when assessing a customer, and will then need to be securely stored for a period of 7 years. In some rare cases, you may need to retrieve this data within 24 hours of a claim being lodged. Given these requirements, which type of AWS storage would deliver the least expensive solution?
	- **Glacier**
	- The recovery rate is a key decider. The record shortage must be; safe, durable, low cost, and the recovery can be slow. All features of Glacier.

- You run a popular photo-sharing website that depends on S3 to store content. Paid advertising is your primary source of revenue. However, you have discovered that other websites are linking directly to the images in your buckets, not to the HTML pages that serve the content. This means that people are not seeing the paid advertising, and you are paying AWS unnecessarily to serve content directly from S3. How might you resolve this issue?
	- **Remove the ability for images to be served public to the site and then use Signed URLs with expiry dates**

- You work for a major news network in Europe. They have just released a new mobile app that allows users to post their photos of newsworthy events in real-time, which are then reviewed by your editors before being copied to your website and made public. Your organization expects this app to grow very quickly, essentially doubling its user base each month. The app uses S3 to store the images, and you are expecting sudden and sizable increases in traffic to S3 when a major news event takes place (as users will be uploading large amounts of content.) You need to keep your storage costs to a minimum, and it does not matter if some objects are lost. With these factors in mind, which storage media should you use to keep costs as low as possible?
	- **S3-OneZone-IA**
	- The key driver here is cost, so an awareness of cost is necessary to answer this. 
		- S3-RRS($0.024/GB)>S3($0.023)>S3 standard IA($0.0125)>S3 One-Zone-IA($0.01)>Glacier($0.004)
	- Glacier cannot be considered as it is not intended for direct access. Of course you spotted that RRS is being deprecated, and there is no such thing as S3 – Provisioned IOPS. In this case OneZone

- You work for a busy digital marketing company who currently store their data on-premise. They are looking to migrate to AWS S3 and to store their data in buckets. Each bucket will be named after their individual customers, followed by a random series of letters and numbers. Once written to S3 the data is rarely changed, as it has already been sent to the end customer for them to use as they see fit. However, on some occasions, customers may need certain files updated quickly, and this may be for work that has been done months or even years ago. You would need to be able to access this data immediately to make changes in that case, but you must also keep your storage costs extremely low. The data is not easily reproducible if lost. Which S3 storage class should you choose to minimize costs and to maximize retrieval times?
	- **S3-IA**
	- The need to immediate access is an important requirement along with cost. Glacier has a long recovery time at a low cost or a shorter recovery time at a high cost, and 1Zone-IA has a lower Availability level which means that it may not be available when needed.

- You work for a major news network in Europe. They have just released a new mobile app that allows users to post their photos of newsworthy events in real-time. Your organization expects this app to grow very quickly, essentially doubling its user base each month. The app uses S3 to store the images, and you are expecting sudden and sizable increases in traffic to S3 when a major news event takes place (as users will be uploading large amounts of content.) You need to keep your storage costs to a minimum, and **you are happy to temporally lose access to up to 0.1% of uploads per year**. With these factors in mind, which storage media should you use to keep costs as low as possible?
	- **S3-IA**
	- S3-RRS($0.024/GB)>S3($0.023)>S3 standard IA($0.0125)>S3 One-Zone-IA($0.01)>Glacier($0.004)
	- Glacier cannot be considered as it is not intended for direct access. 3 has an availability of 99.99%, S3-IA has an availability of 99.9% while S3-1Zone-IA only has 99.5%.

- You work for a manufacturing company that operate a hybrid infrastructure with systems located both in a local data center and in AWS, connected via AWS Direct Connect. Currently, all **on-premise** servers are backed up to a local NAS, but your CTO wants you to decide on the best way to **store copies of these backups** in AWS. He has asked you to propose a solution which will provide **access to the files within milliseconds** should they be needed, but at the same time minimizes cost. As these files will be copies of backups stored on-premise, **availability is not as critical as durability.** Choose the best option from the following which meets the brief.
	- **S3-IA**
	- Cost : S3-RRS($0.024/GB)>S3($0.023)>S3 standard IA($0.0125)>S3 One-Zone-IA($0.01)>Glacier($0.004)
	- Duralility : ALL SAME 11 9's
	- Availability : S3(99.99%)=S3-IA(99.9%)>S3 OneZone-IA(99.5%)

- You need to use an object-based storage solution to store your critical, **non-replaceable** data in a cost-effective way. This data will be **frequently updated** and will need some form of version control enabled on it. Which S3 storage solution should you use?
	- **S3**
	-  The 1st excludes anything the has reduced durability, the second excluded anything with long recall, reduced availability, or billing based on infrequent access.

- You have launched a travel photo sharing website using Amazon S3 to serve high-quality photos to visitors of your website. After a few days, you found out that there are **other travel websites linking and using your photos**. This resulted in financial losses for your business.    
What is an effective method to mitigate this issue?
	- **A) Configure your S3 bucket to remove public read access and use pre-signed URLs with expiry dates.**

- A Solutions Architect is designing an online medical system in AWS which will store sensitive Personally Identifiable Information (PII) of the users in an Amazon S3 bucket. **Both the master keys and the unencrypted data should never be sent to AWS** to comply with the strict compliance and regulatory requirements of the company.    
Which **S3 encryption technique** should the Architect use?
	- **A) Use S3 client-side encryption with a client-side master key.**
	- Client-side encryption is the act of encrypting data **before** sending it to Amazon S3.
	- Using S3 client-side encryption with a KMS-managed customer master key : is incorrect because in client-side encryption with a KMS-managed customer master key, you provide an AWS KMS customer master key ID (CMK ID) to AWS.
	- Using S3 server-side encryption with customer provided key **(SSE-C)** : is incorrect. you have to use client-side encryption in order to encrypt the data first before sending to AWS. For the S3 server-side encryption with customer-provided key (SSE-C), you actually provide the encryption key as part of your request to upload the object to S3. Using this key, Amazon S3 manages both the encryption (as it writes to disks) and decryption (when you access your objects).

- A Solutions Architect is hosting a website in an Amazon S3 bucket named `tutorialsdojo`. The users load the website using the following URL: `http://tutorialsdojo.s3-website-us-east-1.amazonaws.com` and there is a new requirement to add a JavaScript on the webpages in order to make authenticated HTTP GET requests against the same bucket by using the Amazon S3 API endpoint (`tutorialsdojo.s3.amazonaws.com`). Upon testing, you noticed that the **web browser blocks JavaScript from allowing those requests**.    
Which of the following options is the MOST suitable solution that you should implement for this scenario?
	- **A) Enable Cross-origin resource sharing(CORS) configuration in the bucket**

- In a government agency that you are working for, you have been assigned to put confidential tax documents on AWS cloud. However, there is a concern from a security perspective on what can be put on AWS.     
What are the features in AWS that can ensure **data security for your confidential documents?** (Choose 2)
	- **S3 client-side and server-side encryption**

- You are working for an advertising company as their Senior Solutions Architect handling the S3 storage data. Your company has terabytes of data sitting on AWS S3 standard storage class, which accumulates significant operational costs. The management wants to cut down on the cost of their cloud infrastructure so you were instructed to switch to Glacier to lessen the cost per GB storage.    
The Amazon **Glacier storage service is primarily used for** which use case? (Choose 2) 
	- **A1) Storing Data archives**
	- **A2) Storing infrequently accessed data**
	- **Storing cached session data** : is incorrect because this is the main use case for **ElastiCache**
	- **Used as a data warehouse** : is incorrect because data warehousing is the main use case of **Amazon Redshift**.
	
- You are building a transcription service for a company in which a fleet of EC2 worker instances processes an uploaded audio file and generates a text file as an output. You must store both of these frequently accessed files in the same durable storage until the text file is retrieved by the uploader. Due to an expected **surge in demand**, you have to ensure that **the storage is scalable and can be retrieved within minutes**.
Which storage option in AWS can you use in this situation, which is both **cost-efficient and scalable**?
	- **A) A single Amazon S3 bucket** :
	- **Multiple Amazon EBS volume with snapshots** and **Multiple instance stores** : are incorrect because these services do not provide durable storage.

- You are working for a litigation firm as the Data Engineer for their case history application. You need to keep track of all the cases your firm has handled. The static assets like .jpg, .png, and .pdf files are stored in S3 for cost efficiency and high durability. As these files are critical to your business, you want to keep track of what's happening in your S3 bucket. You found out that S3 has an event notification whenever a delete or write operation happens within the S3 bucket.   
What are the possible **Event Notification destinations available for S3 buckets**? (Choose 2)
	- **A1) SQS**
	- **A2) Lambda function**
	- Amazon S3 supports the following destinations where it can publish events:
		- **Amazon Simple Notification Service (Amazon SNS) topic** - A web service that coordinates and manages the delivery or sending of messages to subscribing endpoints or clients.
		- **Amazon Simple Queue Service (Amazon SQS) queue** - Offers reliable and scalable hosted queues for storing messages as they travel between computer.
		- **AWS Lambda** - AWS Lambda is a compute service where you can upload your code and the service can run the code on your behalf using the AWS infrastructure. You package up and upload your custom code to AWS Lambda when you create a Lambda function
	- Steps to trigger event notifications from S3
		- Step1 : Create the queue, topic, or Lambda function (which I’ll call the target for brevity) if necessary.
		- Step2 : Grant S3 permission to publish to the target or invoke the Lambda function.
		- Step3 : Arrange for your application to be invoked in response to activity on the target. As you will see in a moment, you have several options here.
		- Step4 : Set the bucket’s Notification Configuration to point to the target.

- A start-up company that offers an intuitive financial data analytics service has consulted you about their AWS architecture. They have a fleet of Amazon EC2 worker instances that process financial data and then outputs reports which are used by their clients. You must store the generated report files in a **durable storage**. The number of files to be stored can grow over time as the start-up company is expanding rapidly overseas and hence, they also need a way to d**istribute the reports faster to clients located across the globe**.     
Which of the following is a **cost-efficient and scalable storage option** that you should use for this scenario?
	- **A) Use Amazon S3 as the data storage and CloudFront as the CDN.**

- For data privacy, a healthcare company has been asked to comply with the Health Insurance Portability and Accountability Act (HIPAA). They have been told that all of the data being backed up or stored on **Amazon S3 must be encrypted**.    
What is the best option to do this? (Choose 2)
	- **A1) Enable Server-Side Encryption on an S3 bucket to make use of AES-256 encryption.**
	- **A2) Before sending the data to Amazon S3 over HTTPS, encrypt the data locally first using your own encryption keys.**
	- A1. Server-Side Encryption keys
		- Use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3)
		- Use Server-Side Encryption with AWS KMS-Managed Keys (SSE-KMS)
		- Use Server-Side Encryption with Customer-Provided Keys (SSE-C)
	- A2. Client-side encryption is the act of encrypting data before sending it to Amazon S3

- Your fellow AWS Engineer has created a new Standard-class S3 bucket to store financial reports that are not frequently accessed but should be immediately available when an auditor requests for it. To save costs, you changed the storage class of the S3 bucket from Standard to Infrequent Access storage class.       
In Amazon **S3 Standard - Infrequent Access** storage class, which of the following statements are true? (Choose 2)
	- **A1) It is designed for data that requires rapid access when needed.**
	- **A2) It is designed for data that is accessed less frequently.**
	- S3 Standard-IA UseCase : long-term storage, backups, and as a data store for disaster recovery
	- S3 Standard-IA Features :
		- Same low latency and high throughput performance of Standard
		- Designed for durability of 99.999999999% of objects
		- Designed for 99.9% availability over a given year
		- Backed with the Amazon S3 Service Level Agreement for availability
		- Supports SSL encryption of data in transit and at rest
		- Lifecycle management for automatic migration of objects
	- **It is the best storage option to store noncritical and reproducible data** : is incorrect as it actually refers to Amazon S3 - Reduced Redundancy Storage (**RRS**). 

- An online stocks trading application that stores financial data in an S3 bucket has a lifecycle policy that moves older data to **Glacier** every month. There is a strict compliance requirement where a surprise audit can happen at anytime and you should be able to **retrieve** the required data **in under 15 minutes** under all circumstances. Your manager instructed you to ensure that retrieval capacity is available when you need it and should **handle up to 150 MB/s of retrieval throughput**.    
Which of the following should you do to meet the above requirement? (Choose 2)
	- **A1) Use Expedited Retrieval to access the financial data**
	- **A2) Purchase provisioned retrieval capacity**
	- **Expedited retrievals(긴급검색)**
		- 긴급 검색은 가끔 발생하는 소량의 아카이브에 대한 급한 요청에 최적화되어 있습니다. 긴급 검색을 사용하면 가장 큰 아카이브(250MB 이상)를 제외하고 모든 아카이브에 대해 보통 **1~5분 이내**에 데이터에 액세스할 수 있습니다. 애플리케이션이나 워크로드에서 필요할 때 긴급 검색을 사용할 수 있게 보장해야 하는 경우에는 **프로비저닝 용량**을 사용하는 것을 고려해야 합니다.
	- **Provisioned capacity(프로비저닝 용량)**
		- ensures that your retrieval capacity for expedited retrievals is available when you need it. Each unit of capacity provides that at least three expedited retrievals can be performed **every five minutes and provides up to 150 MB/s of retrieval throughput**. You should purchase provisioned retrieval capacity if your workload requires highly reliable and predictable access to a subset of your data **in minutes**. Without provisioned capacity Expedited retrievals are accepted, except for rare situations of unusually high demand. However, **if you require access to Expedited retrievals under all circumstances, you must purchase provisioned retrieval capacity.**
	- **Bulk Retrieval(대량검색)**
		- 대량 검색은 S3 Glacier에서 가장 저렴한 검색 옵션으로 이를 통해 페타바이트 규모의 데이터도 하루 만에 저렴하게 검색할 수 있습니다. 대량 검색은 보통 **5~12시간 이내**에 완료됩니다.
	- **Range Retrieval(범위검색)**
		- 범위 검색으로 아카이브의 특정 범위를 검색할 수 있습니다. 범위 검색은 Amazon S3 Glacier에서의 일반 검색과 유사합니다. 둘 다 검색 작업이 시작되어야 합니다. 범위 검색을 사용하여 검색 요금을 줄이거나 없앨 수 있습니다(무료로 검색할 수 있는 데이터 양은 어느 정도입니까? 참조). 범위 검색을 수행하도록 선택하는 몇 가지 이유가 있습니다. 예를 들어 여러 개의 파일을 모아 단일 아카이브로 업로드했었을 수 있습니다. 그때 해당 파일의 일부를 검색해야 할 수 있고 그 경우 필요한 파일을 포함하는 아카이브 범위만 검색할 수 있습니다. 범위 검색을 수행하도록 선택하는 다른 이유는 일정 기간 동안 Amazon S3 Glacier에서 다운로드한 데이터 양을 관리하기 위한 것입니다. S3 Glacier에서 데이터를 검색하도록 요청하면 아카이브에 대한 검색 작업이 시작됩니다. 검색 작업이 완료되면 24시간 동안 데이터를 다운로드하거나 Amazon Elastic Compute Cloud(Amazon EC2)를 사용해 액세스할 수 있습니다. 검색된 데이터는 24시간 동안 다운로드 가능합니다. 따라서 다운로드 스케줄을 관리하기 위해 부분적으로 아카이브를 검색할 수 있습니다.
	- **Amazon Glacier Select** : is incorrect because this is not an archive retrieval option and is primarily used to perform filtering operations using simple SQL directly on your data archive in Glacier.

- You are working as a Solutions Architect for a multinational financial firm. They have a global online trading platform in which the users from all over the world regularly upload terabytes of transactional data to a centralized S3 bucket.  What AWS feature should you use in your present system to improve throughput and ensure **consistently fast data transfer to the Amazon S3 bucket, regardless of your user's location**?
	- **A) Amazon S3 Transfer Acceleration**
	- Amazon S3 Transfer Acceleration은 거리가 먼 클라이언트와 S3 버킷 간에 파일을 빠르고, 쉽고, 안전하게 전송할 수 있게 해줍니다. Transfer Acceleration은 전 세계적으로 분산되어 있는 Amazon CloudFront의 엣지 로케이션을 활용합니다. 엣지 로케이션에 도착한 데이터는 최적화된 네트워크 경로를 통해 Amazon S3로 라우팅됩니다.
	- **CloudFront Origin Access Identity** : Amazon S3 버킷에서 제공하는 콘텐츠에 대한 액세스를 제한하려면 CloudFront 서명된 URL 또는 서명된 쿠키를 만들어 Amazon S3 버킷에서 파일에 대한 액세스를 제한하고, OAI(원본 액세스 ID)라는 특별한 CloudFront 사용자를 만들어 배포와 연결합니다. 그런 다음 CloudFront가 OAI를 사용하여 사용자에 액세스하고 파일을 제공할 수 있지만, 사용자는 S3 버킷에 대한 직접 URL을 사용하여 파일에 액세스할 수 없도록 권한을 구성합니다. 이 단계를 수행하면 CloudFront를 통해 제공하는 파일에 대한 액세스를 안전하게 유지할 수 있습니다.

- An Architect is managing a data analytics application which exclusively uses Amazon S3 as its data storage. For the past few weeks, the application works as expected until a new change was implemented to increase the rate at which the application **updates** its data. There have been reports that **outdated data intermittently appears when the application accesses objects from S3 bucket**. The development team investigated the application logic and didn’t find any issues.    
Which of the following is the MOST likely cause of this issue?
	- **A) The data analytics application is designed to fetch objects from the S3 bucket using parallel requests.**
	- Amazon S3 provides **read-after-write consistency for PUTS** of new objects in your S3 bucket in all regions with one caveat: if you make a HEAD or GET request to the key name (to find if the object exists) before creating the object, Amazon S3 provides eventual consistency for read-after-write. Amazon S3 offers **eventual consistency for overwrite PUTS and DELETES** in all regions.
	- Amazon S3’s support for parallel requests means you can scale your S3 performance by the factor of your compute cluster, without making any customizations to your application. Amazon S3 does not currently support Object Locking. If two PUT requests are simultaneously made to the same key, the request with the latest timestamp wins. If this is an issue, you will need to build an object-locking mechanism into your application.

- You are working for a large IT consultancy company as a Solutions Architect. One of your clients is launching a file sharing web application in AWS which requires a **durable storage service for hosting their static contents** such as PDFs, Word Documents, high resolution images and many others.    
Which type of storage service should you use to meet this requirement?
	- **A) Amazon S3**
	- **Temporary storage** for needs such as scratch disks, buffers, queues, caches : **Amazon Local Instance Store**
	- **Multi-instance storage** : **EFS** (EBS can only attach to one instance)
	- **Static data or web content** : **S3, EFS**

- One of your clients is leveraging on Amazon S3 in the ap-southeast-1 region to store their training videos for their employee onboarding process. The client is storing the videos using the Standard Storage class.    
Where are your client's training videos replicated?
	- **A) Multiple facilities in ap-southeast-1**
	- Amazon S3 replicates the data to multiple facilities in the same region where it is located.

- Your company has an e-commerce application that saves the transaction logs to an S3 bucket. You are instructed by the CTO to configure the application to **keep the transaction logs for one month for troubleshooting purposes, and then afterwards, purge the logs**. What should you do to accomplish this requirement?
	- **A) Configure the lifecycle configuration rules on the Amazon S3 bucket to purge the transaction logs after a month**

- You are a new Solutions Architect in a large insurance firm. To maintain compliance with HIPAA laws, all data being backed up or stored on **Amazon S3 needs to be encrypted at rest**. In this scenario, what is the best method of encryption for your data, assuming S3 is being used for storing financial-related data? (Choose 2)
	- **A1) Encrypt the data using your own encryption keys then copy the data to Amazon S3 over HTTPS endpoints.**
	- **A2) Enable SSE on an S3 bucket to make use of AES-256 encryption**
	- Data protection refers to protecting data while in-transit (as it travels to and from Amazon S3) and at rest (while it is stored on disks in Amazon S3 data centers). You can protect data in transit by using SSL or by using client-side encryption. You have the following options for protecting data at rest in Amazon S3.
	- Use Server-Side Encryption – You request Amazon S3 to encrypt your object before saving it on disks in its data centers and decrypt it when you download the objects.
	- Use Client-Side Encryption – You can encrypt data client-side and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, the encryption keys, and related tools.

- A document sharing website is using AWS as its cloud infrastructure. Free users can upload a total of 5 GB data while premium users can upload as much as 5 TB. Their application uploads the user files, which can have a max file size of 1 TB, to an S3 Bucket.    
In this scenario, what is the best way for the application to **upload the large files in S3**?
	- **A) Use Multipart Upload**
	- **Using AWS Import/Export** : is incorrect because Import/Export is similar to AWS Snowball in such a way that it is meant to be used as a migration tool

- You are a new Solutions Architect working for a financial company. Your manager wants to have the ability to automatically transfer **obsolete data from their S3 bucket to a low cost storage system in AWS**.
What is the best solution you can provide to them?
	- **A) Use Lifecycle Policies in S3 to move obsolete data to Glacier**
	- **Transition actions** – In which you define when objects transition to another storage class. For example, you may choose to transition objects to the STANDARD_IA (IA, for infrequent access) storage class 30 days after creation, or archive objects to the GLACIER storage class one year after creation.
	- **Expiration actions** – In which you specify when the objects expire. Then Amazon S3 deletes the expired objects on your behalf.

- A music company is storing data on Amazon Simple Storage Service (S3). The company’s security policy requires that **data are encrypted at rest**. Which of the following methods can achieve this? (Choose 2)
	- **A1) Use Amazon S3 server-side encryption with customer-provided keys. 
	- **A2) Encrypt the data on the client-side before ingesting to Amazon S3 using their own master key.**

- You are working as a Cloud Engineer for a top aerospace engineering firm. One of your tasks is to set up a document storage system using S3 for all of the engineering files. In Amazon S3, which of the following statements are true? (Choose 2)
	- **A1) The total volume of data and numver of objects you can store are unlimited.**
	- **A2) The largest object that can be uploaded in a single PUT is 5 GB**
	- **S3 is an object storage service that provides file system access semantics (such as strong consistency and file locking), and concurrently-accessible storage** : is incorrect because although S3 is indeed an object storage service, it does not provide file system access semantics. EFS provides this feature.
