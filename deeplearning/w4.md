## 2-4. Neural Networks-4 ppt
### Contents
- 2/20
- 각 항목에 대한 NN이 바뀐다. 

### Regreggion
- 3/20 : Sigmoid를 쓰면 y는 0~1이 나온다. 그러면 regression 못푸나???
- 4/20
- solution1) 출력값을 0,1로 정규화 
  - 내가 갖고 있는 -100 ~ 100 y를 linear transform해서 0~1 y’로 
  - 그러나 이 방법은 많이 사용하지 않는다. 
- solution2) 그 대신에 활성화함수를 빼고 linear activation function을 사용합니다. 
  - Y=sigmoid(net) 에서 y=net로 그냥 출력해봐요 
  - 주의! Output Layer만 쓰고 Hidden Layer에는 쓰지 않습니다. 
  - 활성화함수는 linear transform -> non linear transfor한다. 이는 hidden layer의 역할. 
  - 출력레이어에서 linear transform을 안써도 되나? : 히든레이어에서 충분히 non linear transform을 충분히 하자. 출력값의 range를 보기 위해서. 
  - 만약 Hidden Layer에서 non linear를 실행하지 않는다면? 
    - 히든 레이어를 두개 사용했지만 히든레이어1에서 Linear activation function을 쓰게 되면 히든 레이어는 존재하지 않는 것과 똑같게 된다. 

### Binary-Class Classification
- 5/20
- 이걸 NN으로 학습하려면 문제가 생긴다. 
  - P1, NN은 Real number가 아닌 norminal value(Red, Black, 대전..)를 다루지 못한다.
  - P2, 학습을 위한 에러함수. 선형함수 할때는 Error 함수로 MSE를 썼다. 그런데, classification에서도 MSE를 쓸 수 있을까? 
- 6/20 P1 해결
- Norminal value에 대해 training data를 preprocessing한다 (0,1로)
- Binary-Class Classification 함수에서는 활성화함수로 sigmoid를 쓴다. 
  - sigmoid는 0,1이 나오는 함수. 
  - 우리는 0,1이 나오면 된다.(red 1 black 0)
- 7/20 P2
- sigmoid와 MSE를 같이 사용하지 않는다. 활성화함수의 시그모이드함수와 에러함수로 MSE를 사용하는 것은 안된다. NN이 학습이 되지않는 운이 나쁜 상태가 발생합니다. 
- MSE의 작동방식 : (타겟값-출력값)을 최소화하는 것. 
- 8/20 P2 해결
- MSE가 아니라 에러함수로 Cross Entropy를 사용하면 된다. 
- 9/20 왜 sigmoid랑 MSE같이쓰면안될까 intro - reminding
- 10/20 왜 sigmoid랑 MSE같이쓰면안될까 intro - reminding
- 노란색 값 : sigmoid를 미분한 값 
- t=1인데 y=0인 경우를 가정해보자. (뉴럴넷이 아주 반대로 알고있는 경우) 이 경우 학습이 되어야하고 gradient가 나와야한다. gradient를 구해보면 -(1-0)0(1-0) = 0!! 학습이 안되게 된다. MSE의 미분값이 (t-h)
- 다시말해서 활성화함수를 시그모이드를 쓰고 MSE를 쓸경우, NN의 결과가 완전히 반대로 나오는 경우 학습이 안된다. 
- 12/20 교차엔트로피
- 반대로 알고 있는 경우 : 델타의 값에(gradient) t=1 y=0을 대입시에 1(1-0)+(1-1)0 = 1로 gradient가 살아있다. 
- 제대로 알고 있는 경우 : t=1 y=1 t=0 y=0인경우 0이 나온다. NN이 제대로 알고 있다는 것이고 update할 필요 없다는 것.
- 13/20 크로스엔트로피:확률을 극대화한다
- 로지스틱 회귀분석을 이해하면 편하다. 
  - 개 1이고 고양이 0이라고 정하고 NN을 돌렸더니 출력값이 0.7이 나왔다. 그러면 우리는 개라고 해석한다. (MSE관점에서 해석한것. 타겟값 - 출력값의 거리가 가까운 것으로 해석) 
  - 확률로 해석하면 어떨까. 개일 확률이 0.7이다. 
- 크로스 엔트로피는 뉴럴넷에서 나오는 출력값을 확률로 이해한것. 
  - RED 개 BLUE 고양이 일떄, 개일떄의 확률을 다 곱하고 * 고양이일때의 확률을 다 곱하면 됨
- 14/20 크로스 엔트로피 수식 
- 크로스 엔트로피란 확률을 극대화하는 w를 찾는 것 
- 그런데 max를 찾는건 좀 힘드니까 그 식에 (-)를 붙이고 min을 붙인다. 
