## 2-4. Neural Networks-4 ppt
### Contents
- 2p
- 각 항목에 대한 NN이 바뀐다. 

### Regreggion
- 3p : Sigmoid를 쓰면 y는 0~1이 나온다. 그러면 regression 못푸나???
- 4p
- solution1) 출력값을 0,1로 정규화 
  - 내가 갖고 있는 -100 ~ 100 y를 linear transform해서 0~1 y’로 
  - 그러나 이 방법은 많이 사용하지 않는다. 
- solution2) 그 대신에 활성화함수를 빼고 linear activation function을 사용합니다. 
  - Y=sigmoid(net) 에서 y=net로 그냥 출력해봐요 
  - 주의! Output Layer만 쓰고 Hidden Layer에는 쓰지 않습니다. 
  - 활성화함수는 linear transform -> non linear transfor한다. 이는 hidden layer의 역할. 
  - 출력레이어에서 linear transform을 안써도 되나? : 히든레이어에서 충분히 non linear transform을 충분히 하자. 출력값의 range를 보기 위해서. 
  - 만약 Hidden Layer에서 non linear를 실행하지 않는다면? 
    - 히든 레이어를 두개 사용했지만 히든레이어1에서 Linear activation function을 쓰게 되면 히든 레이어는 존재하지 않는 것과 똑같게 된다. 

### Binary-Class Classification
- 5p
- 이걸 NN으로 학습하려면 문제가 생긴다. 
  - P1, NN은 Real number가 아닌 norminal value(Red, Black, 대전..)를 다루지 못한다.
  - P2, 학습을 위한 에러함수. 선형함수 할때는 Error 함수로 MSE를 썼다. 그런데, classification에서도 MSE를 쓸 수 있을까? 
- 6p P1 해결
- Norminal value에 대해 training data를 preprocessing한다 (0,1로)
- Binary-Class Classification 함수에서는 활성화함수로 sigmoid를 쓴다. 
  - sigmoid는 0,1이 나오는 함수. 
  - 우리는 0,1이 나오면 된다.(red 1 black 0)
- 7p P2
- sigmoid와 MSE를 같이 사용하지 않는다. 활성화함수의 시그모이드함수와 에러함수로 MSE를 사용하는 것은 안된다. NN이 학습이 되지않는 운이 나쁜 상태가 발생합니다. 
- MSE의 작동방식 : (타겟값-출력값)을 최소화하는 것. 
- 8p P2 해결
- MSE가 아니라 에러함수로 Cross Entropy를 사용하면 된다. 
- 9p 왜 sigmoid랑 MSE같이쓰면안될까 intro - reminding
- 10p 왜 sigmoid랑 MSE같이쓰면안될까 intro - reminding
- 노란색 값 : sigmoid를 미분한 값 
- t=1인데 y=0인 경우를 가정해보자. (뉴럴넷이 아주 반대로 알고있는 경우) 이 경우 학습이 되어야하고 gradient가 나와야한다. gradient를 구해보면 -(1-0)0(1-0) = 0!! 학습이 안되게 된다. MSE의 미분값이 (t-h)
- 다시말해서 활성화함수를 시그모이드를 쓰고 MSE를 쓸경우, NN의 결과가 완전히 반대로 나오는 경우 학습이 안된다. 
- 12p 교차엔트로피
- 반대로 알고 있는 경우 : 델타의 값에(gradient) t=1 y=0을 대입시에 1(1-0)+(1-1)0 = 1로 gradient가 살아있다. 
- 제대로 알고 있는 경우 : t=1 y=1 t=0 y=0인경우 0이 나온다. NN이 제대로 알고 있다는 것이고 update할 필요 없다는 것.
- 13p 크로스엔트로피:확률을 극대화한다
- 로지스틱 회귀분석을 이해하면 편하다. 
  - 개 1이고 고양이 0이라고 정하고 NN을 돌렸더니 출력값이 0.7이 나왔다. 그러면 우리는 개라고 해석한다. (MSE관점에서 해석한것. 타겟값 - 출력값의 거리가 가까운 것으로 해석) 
  - 확률로 해석하면 어떨까. 개일 확률이 0.7이다. 
- 크로스 엔트로피는 뉴럴넷에서 나오는 출력값을 확률로 이해한것. 
  - RED 개 BLUE 고양이 일떄, 개일떄의 확률을 다 곱하고 * 고양이일때의 확률을 다 곱하면 됨
- 14p 크로스 엔트로피 수식 
- 크로스 엔트로피란 확률을 극대화하는 w를 찾는 것 
- 그런데 max를 찾는건 좀 힘드니까 그 식에 (-)를 붙이고 min을 붙인다. 

### Multi-Class Classification 
- 15p 개념 
- 이제는 2개가 아니고 여러개를 분류해야하는 상황
- 문제 : nominal value를 real number로 매핑해야한다. (Red, Yellow, Blue -> 숫자로)
- 16p Nominal Value 처리하기
- 1, 0.5, 0 으로 정하고 sigmoid함수를 활성화함수로 하면 되지 않나요?
- 17p Nominal Value를 1, 0.5, 0으로 정하면 안좋은 이유 
- 레드를 블루라고 얘기하거나 옐로라고 얘기하거나 둘다 순서없이 별로다. 
- MSE의 관점에서는 1을 0.5라고 말해서 틀리는게 0이라고 말해서 틀리는게 더 낫다 라는 그런 암묵적인 규칙이 생겨버린다. Linear 매핑을 한다면..
- 18p 출력을 쪼갠다 
- 출력을 one hot encoding으로 바꿔준다. 
  - one hot encoding : 1이 한개가 있는 값으로 변화시키기. 
- output의 개수는 class의 개수와 같다.(R, Y, B)
- 출력이 0,1이기 떄문에 활성화함수는 교차엔트로피를 쓴다. 
- 노란색 hmm : 근데 바이너리랑 교차엔트로피 모양이 달라요? 
- 19p multi class를 위한 교차 엔트로피 
- 바이너리 클래스도 멀티 클래스니까 세번째거처럼 변경해보자. output을 2개 두기. (1,0) (0,1) 
- 출력 노드가 2개 생기니까 출력에서 각 출력노드의 크로스엔트로피를 더한다. 
- 출력 1개일때 : 똑같은 트레이닝 데이터에 1이 나온다면 / 출력 2개일때 : 데이터는 1,0이 나온다.     
이를 대입해보면 (1 ?)(1-1 * (?)) 이 식과  (1 ?)+ 0 * ? 이 식은 동등하게 되다 
- 20p activation function
- 멀티 클래스 분류에서는 활성화함수에서 시그모이드를 사용하지 않고 소프트맥스 함수를 씁니다. 
- 멀티 클래스 분류에서는 타켓 값을 다 더하면 1이 된다.
- 그러나 시그모이드를 쓰면 위의 조건이 안맞춰진다. 
- 그래서 우리는 소프트맥스 레이어를 씁니다. 
- 21p softmax layer 
- regular layer에서는 : net -> activation function -> 출력
- softmax layer에서는 : net -> exp -> normalize -> 출력
- 모든 tn값을 다 더하면 1이 되는 것처럼, 나오는 출력값을 다 더하면 1이 되도록 정규화를 하자. 
- exponential 왜 쓰나요? : 그렇게하면 수학적 성질이 더 좋아져서요.. classification은 크로스엔트로피를 쓰고 이는 확률 기반이다. 모든 트레이닝 데이터에 대해 NN이 맞출 확률을 다 곱한다. 이걸 뭘 어떻게 하면 exp가 들어간다. 
- 22p multi class Error Function 
- 에러함수로는 교차엔트로피 사용. 
- 정리하자면 
  - input을 one hot encoding으로 nominal value 정리하고 
  - output은 softmax 쓰고 
  - error함수는 교차엔트로피 씁니다. 
- 23p multi-class vs. multi-lavel
- multi-class : output 1 가질수있는 value 여러개
- multi-label : output 여러개 
  - (x11 x12 1 1 1 ), (x21 x22 0 1 1) 이렇게 되는 건데. multi class와 뭔 차이가 있나요? : multiclass는 다 더하면 1입니다. multi label은 다 더하면 1이라는 조건이 없습니다. 
- 24p 그래서 multi label은 output layer에 sigmoid 써도 됩니다. 다 더하면 1이 되어야한다는 조건이 없기 떄문에 
- 25p multi label은 일반 regular layer에 sigmoid 쓰고 Loss Function은 교차 엔트로피 씁니다. 
- 출력값은 독립의 output이기 때문에 (독립적인 binary class가 여러개) 각기의 크로스 엔트로피를 죄다 더한 것을 NN의 에러로 정의를 해서 학습을 시켜야한다. 

### Nominal Inputs
- 26p
- one hot encoding 으로 변환해서 사용한다 












